{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Orchestration with a Planner\n",
    "\n",
    "In this workshop, we will use the planner to design a multi-agent **FLEET** designated for the task of generating a cybersecurity report. The following agents will be included:\n",
    "\n",
    "1. time_keeper\n",
    "2. cyber_collector\n",
    "3. db_reader\n",
    "4. data_analyzer\n",
    "5. security_evaluator\n",
    "6. report_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "logging.getLogger('azure.core.pipeline.policies.http_logging_policy').setLevel(\n",
    "    logging.WARNING)\n",
    "\n",
    "load_dotenv()\n",
    "print(os.getenv('CHAT_MODEL'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(), conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fleet_name = \"internet_threat_analysis\"\n",
    "agent_fleet = []\n",
    "agent_list = project_client.agents.list_agents().data\n",
    "for _agent in agent_list:\n",
    "    if \"group\" in _agent.metadata.keys() and _agent.metadata[\"group\"] == fleet_name:\n",
    "        agent_fleet.append({\"id\": _agent.id, \n",
    "                            \"name\": _agent.name,\n",
    "                            \"description\": _agent.description})\n",
    "        \n",
    "agent_fleet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_client = project_client.inference.get_azure_openai_client(\n",
    "    api_version=\"2024-06-01\")\n",
    "\n",
    "deployment_name = os.environ[\"CHAT_MODEL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"You are an intelligent assistant capable of responding to user queries. When you receive a message from the user, you need to determine whether you can answer it directly or if it would be more appropriate to allocate one of the listed agents to respond.\n",
    "\n",
    "Direct Response: If the question falls within your knowledge and capabilities, you will respond directly. Your response should start with the prefix [__PLANNER__], followed by your answer.\n",
    "\n",
    "Agent Allocation: If the query is better suited for one of the agents, you will allocate the most appropriate agent based on the userâ€™s message. Your response will begin with the prefix [__AGENT__], followed by the agent's name. If the allocated agent is unable to provide a satisfactory answer, you may decide to allocate a different agent that may better address the user's needs.\n",
    "\n",
    "Here is the list of agents you will be working with:\n",
    "[__AGENT_LIST_PLACEHOLDER__]\n",
    "\n",
    "Make sure to consider the descriptions of each agent when deciding which one to allocate. If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer. Your goal is to provide clear and helpful information to the user, whether through your own knowledge or via the correct agent.\n",
    "\n",
    "## Constraints\n",
    "- Do not allocate an agent if you can answer the question directly.\n",
    "- If there is an agent that can possibly answer the question, allocate that agent.\n",
    "- If an agent cannot find sufficient information or provide a relevant response, you may allocate another agent to ensure the user receives the most relevant and informed answer.\n",
    "- If you don't allocate an agent, you must respond with the prefix [__PLANNER__], followed by your response. For example, [__PLANNER__]Hello, I'm here to assist you.;\n",
    "- If you allocate an agent, you must respond with the prefix [__AGENT__], followed by the agent's name, and then [__TASK__] followed by the request to that agent. For example: [__AGENT__]HR Helpdesk;[__TASK__]Get the company benefit policies.;\n",
    "- If you think the user's question is fully answered, respond with the prefix [__TERMINATION__] followed by your response.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "cyber_fleet_str = json.dumps(agent_fleet).replace(\"{\", '{{').replace(\"}\", '}}')\n",
    "print(sys_prompt.replace(\"[__AGENT_LIST_PLACEHOLDER__]\", cyber_fleet_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "addtional_constraint = '''\n",
    "## Guidelines to generate a report\n",
    "1. Start with the time keeper to get the current time.\n",
    "2. Allocate the cyber collector to retrieve latest threat information.\n",
    "3. Use the metric types from the cyber collector to have the db reader fetch database data.\n",
    "4. Use the CSV files from the db reader to have the data analyzer perform analysis, including forecasting and anomaly detection.\n",
    "5. Allocate the security evaluator to create radar chart of evaluation metrics.\n",
    "6. Start report writer to compile information and generate PDF report.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_functions import user_functions\n",
    "from azure.ai.projects.models import FunctionTool, RequiredFunctionToolCall, SubmitToolOutputsAction, ToolOutput\n",
    "\n",
    "functions = FunctionTool(functions=user_functions)\n",
    "\n",
    "\n",
    "def agent_execution(agent_id, task, context):\n",
    "\n",
    "    thread = project_client.agents.create_thread()\n",
    "    print(f\"Created thread, ID: {thread.id}\")\n",
    "\n",
    "    # Create message to thread\n",
    "    message = project_client.agents.create_message(\n",
    "        thread_id=thread.id, role=\"user\", content=f'task: {task} \\n\\n context: {context}')\n",
    "\n",
    "    print(f\"Created message, ID: {message.id}\")\n",
    "\n",
    "    # Create and process assistant run in thread with tools\n",
    "    run = project_client.agents.create_run(\n",
    "        thread_id=thread.id, assistant_id=agent_id)\n",
    "    print(f\"Created run, ID: {run.id}\")\n",
    "\n",
    "    while run.status in [\"queued\", \"in_progress\", \"requires_action\"]:\n",
    "        run = project_client.agents.get_run(thread_id=thread.id, run_id=run.id)\n",
    "\n",
    "        if run.status == \"requires_action\" and isinstance(run.required_action, SubmitToolOutputsAction):\n",
    "            tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "            if not tool_calls:\n",
    "                print(\"No tool calls provided - cancelling run\")\n",
    "                project_client.agents.cancel_run(\n",
    "                    thread_id=thread.id, run_id=run.id)\n",
    "                break\n",
    "\n",
    "            tool_outputs = []\n",
    "            for tool_call in tool_calls:\n",
    "                if isinstance(tool_call, RequiredFunctionToolCall):\n",
    "                    try:\n",
    "                        print(f\"Executing tool call: {tool_call}\")\n",
    "                        output = functions.execute(tool_call)\n",
    "                        tool_outputs.append(\n",
    "                            ToolOutput(\n",
    "                                tool_call_id=tool_call.id,\n",
    "                                output=output,\n",
    "                            )\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing tool_call {tool_call.id}: {e}\")\n",
    "\n",
    "            print(f\"Tool outputs: {tool_outputs}\")\n",
    "            if tool_outputs:\n",
    "                project_client.agents.submit_tool_outputs_to_run(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=tool_outputs\n",
    "                )\n",
    "\n",
    "        print(f\"Current run status: {run.status}\")\n",
    "\n",
    "    print(f\"Run completed with status: {run.status}\")\n",
    "\n",
    "    messages = project_client.agents.list_messages(thread_id=thread.id)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_assistant_content(agent_messages):\n",
    "    contents = []\n",
    "    for message in agent_messages:\n",
    "        if message['role'] == 'assistant':\n",
    "            contents.append(message['content'][0]['text']['value'])\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt.replace(\"[__AGENT_LIST_PLACEHOLDER__]\", cyber_fleet_str) + addtional_constraint\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a cybersecurity report for TrendMicro.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aoai_client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "llm_response = response.choices[0].message.content\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "while \"[__AGENT__]\" in llm_response:\n",
    "    agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "    print(f\"Allocated agent: {agent_name}\")\n",
    "    agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "    print(f\"Agent task: {agent_task}\")\n",
    "    # find agent id from agent fleet based on agent_name\n",
    "    agent_id = None\n",
    "    for agent in agent_fleet:\n",
    "        if agent[\"name\"] == agent_name:\n",
    "            agent_id = agent[\"id\"]\n",
    "            break\n",
    "\n",
    "    if agent_id:\n",
    "        agent_messages = agent_execution(agent_id, agent_task, json.dumps(messages[1:]))\n",
    "        agent_contents = get_assistant_content(agent_messages.data)\n",
    "        if agent_contents:\n",
    "            for agent_content in agent_contents:\n",
    "                messages.append(\n",
    "                    {\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{agent_content}\"})\n",
    "\n",
    "        response = aoai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "        llm_response = response.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": sys_prompt.replace(\"[__AGENT_LIST_PLACEHOLDER__]\", cyber_fleet_str) + addtional_constraint\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Generate a cybersecurity report for TrendMicro.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "response = aoai_client.chat.completions.create(\n",
    "    model=deployment_name,\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000\n",
    ")\n",
    "messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "\n",
    "llm_response = response.choices[0].message.content\n",
    "if \"[__AGENT__]\" in llm_response:\n",
    "    agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "    print(f\"Allocated agent: {agent_name}\")\n",
    "    agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "    print(f\"Agent task: {agent_task}\")\n",
    "    # find agent id from agent fleet based on agent_name\n",
    "    agent_id = None\n",
    "    for agent in agent_fleet:\n",
    "        if agent[\"name\"] == agent_name:\n",
    "            agent_id = agent[\"id\"]\n",
    "            break\n",
    "\n",
    "    if agent_id:\n",
    "        agent_messages = agent_execution(agent_id, agent_task)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{get_assistant_content(agent_messages.data)}\"})\n",
    "        print(messages)\n",
    "\n",
    "        response = aoai_client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "            max_tokens=1000\n",
    "        )\n",
    "\n",
    "        llm_response = response.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "        print(llm_response)\n",
    "        \n",
    "        if \"[__AGENT__]\" in llm_response:\n",
    "            agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "            print(f\"Allocated agent: {agent_name}\")\n",
    "            agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "            print(f\"Agent task: {agent_task}\")\n",
    "            # find agent id from agent fleet based on agent_name\n",
    "            agent_id = None\n",
    "            for agent in agent_fleet:\n",
    "                if agent[\"name\"] == agent_name:\n",
    "                    agent_id = agent[\"id\"]\n",
    "                    break\n",
    "\n",
    "            if agent_id:\n",
    "                agent_messages = agent_execution(agent_id, agent_task)\n",
    "                messages.append({\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{get_assistant_content(agent_messages.data)}\"})\n",
    "                \n",
    "                response = aoai_client.chat.completions.create(\n",
    "                    model=deployment_name,\n",
    "                    messages=messages,\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "\n",
    "                llm_response = response.choices[0].message.content\n",
    "                messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "                \n",
    "                if \"[__AGENT__]\" in llm_response:\n",
    "                    agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "                    print(f\"Allocated agent: {agent_name}\")\n",
    "                    agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "                    print(f\"Agent task: {agent_task}\")\n",
    "                    # find agent id from agent fleet based on agent_name\n",
    "                    agent_id = None\n",
    "                    for agent in agent_fleet:\n",
    "                        if agent[\"name\"] == agent_name:\n",
    "                            agent_id = agent[\"id\"]\n",
    "                            break\n",
    "\n",
    "                    if agent_id:\n",
    "                        agent_messages = agent_execution(agent_id, agent_task)\n",
    "                        messages.append({\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{get_assistant_content(agent_messages.data)}\"})\n",
    "                        \n",
    "                        response = aoai_client.chat.completions.create(\n",
    "                            model=deployment_name,\n",
    "                            messages=messages,\n",
    "                            temperature=0.7,\n",
    "                            max_tokens=1000\n",
    "                        )\n",
    "\n",
    "                        llm_response = response.choices[0].message.content\n",
    "                        messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "\n",
    "                        if \"[__AGENT__]\" in llm_response:\n",
    "                            agent_name = llm_response.split(\"[__AGENT__]\")[1].split(\";\")[0]\n",
    "                            print(f\"Allocated agent: {agent_name}\")\n",
    "                            agent_task = llm_response.split(\"[__TASK__]\")[1]\n",
    "                            print(f\"Agent task: {agent_task}\")\n",
    "                            # find agent id from agent fleet based on agent_name\n",
    "                            agent_id = None\n",
    "                            for agent in agent_fleet:\n",
    "                                if agent[\"name\"] == agent_name:\n",
    "                                    agent_id = agent[\"id\"]\n",
    "                                    break\n",
    "\n",
    "                            if agent_id:\n",
    "                                agent_messages = agent_execution(agent_id, agent_task)\n",
    "                                messages.append({\"role\": \"assistant\", \"content\": f\"[__AGENT__({agent_name})]{get_assistant_content(agent_messages.data)}\"})\n",
    "                                \n",
    "                                response = aoai_client.chat.completions.create(\n",
    "                                    model=deployment_name,\n",
    "                                    messages=messages,\n",
    "                                    temperature=0.7,\n",
    "                                    max_tokens=1000\n",
    "                                )\n",
    "\n",
    "                                llm_response = response.choices[0].message.content\n",
    "                                messages.append({\"role\": \"assistant\", \"content\": f\"{llm_response}\"})\n",
    "                        \n",
    "\n",
    "elif \"[__TERMINATION__]\" in llm_response:\n",
    "    print(\"Direct response\")\n",
    "    \n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_messages['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
