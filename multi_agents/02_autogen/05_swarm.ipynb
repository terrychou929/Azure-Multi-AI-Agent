{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'autogen-agentchat==0.4.0.dev8' -qU\n",
    "%pip install 'autogen-ext[openai,azure]==0.4.0.dev8' -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../../.env\")\n",
    "os.environ.get(\"AZURE_OPENAI_ENDPOINT_GPT_4o\")\n",
    "\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"]\n",
    "api_base = os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm\n",
    "`Swarm` implements a team in which agents can hand off task to other agents based on their capabilities. It is a multi-agent design pattern first introduced by OpenAI in an experimental project. The key idea is to let agent delegate tasks to other agents using a special tool call, while all agents share the same message context. This enables agents to make local decisions about task planning, rather than relying on a central orchestrator such as in `SelectorGroupChat`.\n",
    "\n",
    "> `Swarm` is a high-level API. If you need more, control and customization that is not supported by this API, you can take a look,\n",
    "at the [Handoff Pattern](../../core-user-guide/design-patterns/handoffs.ipynb), in the Core API documentation and implement your own version of the Swarm pattern.,\n",
    "\n",
    "\n",
    "### How Does It Work?\n",
    "At its core, the `Swarm` team is a group chat where agents take turn to generate a response. Similar to `SelectorGroupChat` and `RoundRobinGroupChat`, participant agents broadcast their responses so all agents share the same message context.\n",
    "\n",
    "Different from the other two group chat teams, at each turn, the speaker agent is selected based on the most recent `HandoffMessage` message in the context. This naturally requires each agent in the team to be able to generate `HandoffMessage` to signal which other agents that it hands off to.\n",
    "\n",
    "For `AssistantAgent`, you can set the handoffs argument to specify which agents it can hand off to. You can use `Handoff` to customize the message content and handoff behavior.\n",
    "\n",
    "The overall process can be summarized as follows:\n",
    "\n",
    "- Each agent has the ability to generate `HandoffMessage` to signal which other agents it can hand off to. For `AssistantAgent`, this means setting the handoffs argument.\n",
    "\n",
    "- When the team starts on a task, the first speaker agents operate on the task and make localized decision about whether to hand off and to whom.\n",
    "\n",
    "- When an agent generates a `HandoffMessage`, the receiving agent takes over the task with the same message context.\n",
    "\n",
    "- The process continues until a termination condition is met.\n",
    "\n",
    "In this section, we will show you two examples of how to use the `Swarm` team:\n",
    "\n",
    "1. A customer support team with human-in-the-loop handoff.\n",
    "\n",
    "2. An autonomous team for content generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Support Example\n",
    "\n",
    "<Image src='docs/swarm_customer_support.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This system implements a flights refund scenario with two agents:\n",
    "\n",
    "- Travel Agent: Handles general travel and refund coordination.\n",
    "\n",
    "- Flights Refunder: Specializes in processing flight refunds with the `refund_flight` tool.\n",
    "\n",
    "Additionally, we let the user interact with the agents, when agents handoff to \"user\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "1. The Travel Agent initiates the conversation and evaluates the user’s request.\n",
    "\n",
    "2. Based on the request:\n",
    "\n",
    "    - For refund-related tasks, the Travel Agent hands off to the Flights Refunder.\n",
    "\n",
    "    - For information needed from the customer, either agent can hand off to the \"user\".\n",
    "\n",
    "3. The Flights Refunder processes refunds using the `refund_flight` tool when appropriate.\n",
    "\n",
    "4. If an agent hands off to the \"user\", the team execution will stop and wait for the user to input a response.\n",
    "\n",
    "5. When the user provides input, it’s sent back to the team as a `HandaffMessage`. This message is directed to the agent that originally requested user input.\n",
    "\n",
    "6. The process continues until the Travel Agent determines the task is complete and terminates the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "from autogen_agentchat.teams import Swarm\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import HandoffMessage\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.task import Console, HandoffTermination, TextMentionTermination\n",
    "\n",
    "\n",
    "def refund_flight(flight_id: str) -> str:\n",
    "    \"\"\"Refund a flight\"\"\"\n",
    "    return f\"Flight {flight_id} refunded\"\n",
    "\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "travel_agent = AssistantAgent(\n",
    "    \"travel_agent\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"flights_refunder\", \"user\"],\n",
    "    system_message=\"\"\"You are a travel agent.\n",
    "    The flights_refunder is in charge of refunding flights.\n",
    "    If you need information from the user, you must first send your message, then you can handoff to the user.\n",
    "    Use TERMINATE when the travel planning is complete.\"\"\",\n",
    ")\n",
    "\n",
    "flights_refunder = AssistantAgent(\n",
    "    \"flights_refunder\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"travel_agent\", \"user\"],\n",
    "    tools=[refund_flight],\n",
    "    system_message=\"\"\"You are an agent specialized in refunding flights.\n",
    "    You only need flight reference numbers to refund a flight.\n",
    "    You have the ability to refund a flight using the refund_flight tool.\n",
    "    If you need information from the user, you must first send your message, then you can handoff to the user.\n",
    "    When the transaction is complete, handoff to the travel agent to finalize.\"\"\",\n",
    ")\n",
    "\n",
    "termination = HandoffTermination(target=\"user\") | TextMentionTermination(\"TERMINATE\")\n",
    "team = Swarm([travel_agent, flights_refunder], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await team.reset()  # Reset the team for the next run.\n",
    "\n",
    "task = \"I need to refund my flight.\"\n",
    "\n",
    "\n",
    "async def run_team_stream() -> None:\n",
    "    task_result = await Console(team.run_stream(task=task))\n",
    "    last_message = task_result.messages[-1]\n",
    "\n",
    "    while isinstance(last_message, HandoffMessage) and last_message.target == \"user\":\n",
    "        user_message = input(\"User: \")\n",
    "\n",
    "        task_result = await Console(\n",
    "            team.run_stream(task=HandoffMessage(\n",
    "                source=\"user\", target=last_message.source, content=user_message))\n",
    "        )\n",
    "        last_message = task_result.messages[-1]\n",
    "\n",
    "\n",
    "await run_team_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Research Example\n",
    "\n",
    "<Image src='docs/swarm_stock_research.svg'>\n",
    "\n",
    "This system is designed to perform stock research tasks by leveraging four agents:\n",
    "\n",
    "- **Planner**: The central coordinator that delegates specific tasks to specialized agents based on their expertise. The planner ensures that each agent is utilized efficiently and oversees the overall workflow.\n",
    "\n",
    "- **Financial Analyst**: A specialized agent responsible for analyzing financial metrics and stock data using tools such as get_stock_data.\n",
    "\n",
    "- **News Analyst**: An agent focused on gathering and summarizing recent news articles relevant to the stock, using tools such as get_news.\n",
    "\n",
    "- **Writer**: An agent tasked with compiling the findings from the stock and news analysis into a cohesive final report.\n",
    "\n",
    "## Workflow\n",
    "1. The Planner initiates the research process by delegating tasks to the appropriate agents in a step-by-step manner.\n",
    "\n",
    "2. Each agent performs its task independently and appends their work to the shared message thread/history. Rather than directly returning results to the planner, all agents contribute to and read from this shared message history. When agents generate their work using the LLM, they have access to this shared message history, which provides context and helps track the overall progress of the task.\n",
    "\n",
    "3. Once an agent completes its task, it hands off control back to the planner.\n",
    "\n",
    "4. The process continues until the planner determines that all necessary tasks have been completed and decides to terminate the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_stock_data(symbol: str) -> Dict[str, Any]:\n",
    "    \"\"\"Get stock market data for a given symbol\"\"\"\n",
    "    return {\"price\": 180.25, \"volume\": 1000000, \"pe_ratio\": 65.4, \"market_cap\": \"700B\"}\n",
    "\n",
    "\n",
    "async def get_news(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Get recent news articles about a company\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"title\": \"Tesla Expands Cybertruck Production\",\n",
    "            \"date\": \"2024-03-20\",\n",
    "            \"summary\": \"Tesla ramps up Cybertruck manufacturing capacity at Gigafactory Texas, aiming to meet strong demand.\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Tesla FSD Beta Shows Promise\",\n",
    "            \"date\": \"2024-03-19\",\n",
    "            \"summary\": \"Latest Full Self-Driving beta demonstrates significant improvements in urban navigation and safety features.\",\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"Model Y Dominates Global EV Sales\",\n",
    "            \"date\": \"2024-03-18\",\n",
    "            \"summary\": \"Tesla's Model Y becomes best-selling electric vehicle worldwide, capturing significant market share.\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "planner = AssistantAgent(\n",
    "    \"planner\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"financial_analyst\", \"news_analyst\", \"writer\"],\n",
    "    system_message=\"\"\"You are a research planning coordinator.\n",
    "    Coordinate market research by delegating to specialized agents:\n",
    "    - Financial Analyst: For stock data analysis\n",
    "    - News Analyst: For news gathering and analysis\n",
    "    - Writer: For compiling final report\n",
    "    Always send your plan first, then handoff to appropriate agent.\n",
    "    Handoff to a single agent at a time.\n",
    "    Use TERMINATE when research is complete.\"\"\",\n",
    ")\n",
    "\n",
    "financial_analyst = AssistantAgent(\n",
    "    \"financial_analyst\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"planner\"],\n",
    "    tools=[get_stock_data],\n",
    "    system_message=\"\"\"You are a financial analyst.\n",
    "    Analyze stock market data using the get_stock_data tool.\n",
    "    Provide insights on financial metrics.\n",
    "    Always handoff back to planner when analysis is complete.\"\"\",\n",
    ")\n",
    "\n",
    "news_analyst = AssistantAgent(\n",
    "    \"news_analyst\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"planner\"],\n",
    "    tools=[get_news],\n",
    "    system_message=\"\"\"You are a news analyst.\n",
    "    Gather and analyze relevant news using the get_news tool.\n",
    "    Summarize key market insights from news.\n",
    "    Always handoff back to planner when analysis is complete.\"\"\",\n",
    ")\n",
    "\n",
    "writer = AssistantAgent(\n",
    "    \"writer\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[\"planner\"],\n",
    "    system_message=\"\"\"You are a financial report writer.\n",
    "    Compile research findings into clear, concise reports.\n",
    "    Always handoff back to planner when writing is complete.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define termination condition\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "termination = text_termination\n",
    "\n",
    "research_team = Swarm(\n",
    "    participants=[planner, financial_analyst, news_analyst, writer], termination_condition=termination\n",
    ")\n",
    "\n",
    "task = \"Conduct market research for TSLA stock\"\n",
    "await Console(research_team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Selector Function\n",
    "Often times we want better control over the selection process. To this end, we can set the `selector_func` argument with a custom selector function to override the default model-based selection. For instance, we want the Planning Agent to speak immediately after any specialized agent to check the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector_func(messages: Sequence[AgentMessage]) -> str | None:\n",
    "    if messages[-1].source != planning_agent.name:\n",
    "        return planning_agent.name\n",
    "    return None\n",
    "\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_func=selector_func,\n",
    ")\n",
    "\n",
    "await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `on_messages_stream()` method returns an asynchronous generator that yields each inner message generated by the agent, and the last item is the final response message in the `chat_message` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.task import Console\n",
    "\n",
    "# Use `asyncio.run(single_agent_team.reset())` when running in a script.\n",
    "await single_agent_team.reset()  # Reset the team for the next run.\n",
    "# Use `asyncio.run(single_agent_team.run_stream(task=\"What is the weather in Seattle?\"))` when running in a script.\n",
    "await Console(\n",
    "    single_agent_team.run_stream(task=\"What is the weather in Seattle?\")\n",
    ")  # Stream the messages to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "httpx.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Pattern\n",
    "Now we will create a team with two agents that implements the Reflection pattern, which is a multi-agent design pattern that uses a critic agent to evaluate the responses of a primary agent.\n",
    "\n",
    "See how the reflection pattern works using the Core API.\n",
    "\n",
    "In this example, we will use the `AssistantAgent` agent class for both the primary and critic agents. We will use both the `TextMentionTermination` and `MaxMessageTermination` termination conditions together to stop the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.task import MaxMessageTermination, TextMentionTermination\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Create the primary agent.\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    ")\n",
    "\n",
    "# Create the critic agent.\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that stops the task if the critic approves.\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "# Define a termination condition that stops the task after 5 messages.\n",
    "max_message_termination = MaxMessageTermination(5)\n",
    "# Combine the termination conditions using the `|`` operator so that the\n",
    "# task stops when either condition is met.\n",
    "termination = text_termination | max_message_termination\n",
    "\n",
    "# Create a team with the primary and critic agents.\n",
    "reflection_team = RoundRobinGroupChat(\n",
    "    [primary_agent, critic_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"Write a short poem about fall season.\")))` when running in a script.\n",
    "await Console(\n",
    "    reflection_team.run_stream(task=\"Write a short poem about fall season.\")\n",
    ")  # Stream the messages to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming Team\n",
    "Let’s run the team again with a new task while keeping the context about the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the poem in Chinese Tang poetry style.\n",
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"将这首诗用中文唐诗风格写一遍。\")))` when running in a script.\n",
    "await Console(reflection_team.run_stream(task=\"将这首诗用中文唐诗风格写一遍。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the poem in Spanish.\n",
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"Write the poem in Spanish.\")))` when running in a script.\n",
    "await Console(reflection_team.run_stream(task=\"Write the poem in Spanish.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming A Previous Task\n",
    "We can call `run()` or `run_stream()` methods without setting the task again to resume the previous task. The team will continue from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `asyncio.run(Console(reflection_team.run_stream()))` when running in a script.\n",
    "await Console(reflection_team.run_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pause for User Input\n",
    "Often times, team needs additional input from the application (i.e., user) to continue processing the task. We will show two possible ways to do it:\n",
    "\n",
    "- Set the maximum number of turns such that the team stops after the specified number of turns.\n",
    "\n",
    "- Use the `HandoffTermination` termination condition.\n",
    "\n",
    "You can also use custom termination conditions, see Termination Conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Number of Turns\n",
    "This is the simplest way to pause the team for user input. For example, you can set the maximum number of turns to 1 such that the team stops right after the first agent responds. This is useful when you want the user to constantly engage with the team, such as in a chatbot scenario.\n",
    "\n",
    "Simply set the `max_turns` parameter in the `RoundRobinGroupChat()` constructor.\n",
    "\n",
    "```python\n",
    "team = RoundRobinGroupChat([...], max_turns=1)\n",
    "```\n",
    "\n",
    "Once the team stops, the turn count will be reset. When you resume the team, it will start from 0 again.\n",
    "\n",
    "Note that `max_turn` is specific to the team class and is currently only supported by `RoundRobinGroupChat`, `SelectorGroupChat`, and `Swarm`. When used with termination conditions, the team will stop when either condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Handoff to Pause Team\n",
    "You can use the `HandoffTermination` termination condition to stop the team when an agent sends a `HandoffMessage` message.\n",
    "\n",
    "Let’s create a team with a single `AssistantAgent` agent with a handoff setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.agents import AssistantAgent, Handoff\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.task import HandoffTermination, TextMentionTermination\n",
    "\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Create a lazy assistant agent that always hands off to the user.\n",
    "lazy_agent = AssistantAgent(\n",
    "    \"lazy_assistant\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n",
    "    system_message=\"Always transfer to user when you don't know the answer. Respond 'TERMINATE' when task is complete.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that checks for handoff message targetting helper and text \"TERMINATE\".\n",
    "handoff_termination = HandoffTermination(target=\"user\")\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "termination = handoff_termination | text_termination\n",
    "\n",
    "# Create a single-agent team.\n",
    "lazy_agent_team = RoundRobinGroupChat(\n",
    "    [lazy_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.task import Console\n",
    "\n",
    "# Use `asyncio.run(Console(lazy_agent_team.run_stream(task=\"What is the weather in New York?\")))` when running in a script.\n",
    "await Console(lazy_agent_team.run_stream(task=\"What is the weather in New York?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `asyncio.run(Console(lazy_agent_team.run_stream(task=\"It is raining in New York.\")))` when running in a script.\n",
    "await Console(lazy_agent_team.run_stream(task=\"It is raining in New York.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
