{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'autogen-agentchat==0.4.0.dev8' -qU\n",
    "%pip install 'autogen-ext[openai,azure]==0.4.0.dev8' -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../../.env\")\n",
    "os.environ.get(\"AZURE_OPENAI_ENDPOINT_GPT_4o\")\n",
    "\n",
    "api_key = os.environ[\"AZURE_OPENAI_API_KEY_GPT_4o\"]\n",
    "api_base = os.environ[\"AZURE_OPENAI_ENDPOINT_GPT_4o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selector Group Chat\n",
    "`SelectorGroupChat` implements a team where participants take turns broadcasting messages to all other participants, with the next speaker selected by a generative model (e.g., an LLM) based on the shared context. This enables dynamic and context-aware multi-agent collaboration.\n",
    "\n",
    "SelectorGroupChat provides several key features:\n",
    "\n",
    "- Model-based speaker selection\n",
    "\n",
    "- Configurable participant roles and descriptions\n",
    "\n",
    "- Optional prevention of consecutive turns by the same speaker\n",
    "\n",
    "- Customizable selection prompting\n",
    "\n",
    "- Customizable selection function to override the default model-based selection\n",
    "\n",
    "### How does it work?\n",
    "SelectorGroupChat is a group chat similar to RoundRobinGroupChat, but with a model-based next speaker selection mechanism. When the team receives a task through run() or run_stream(), the following steps are executed:\n",
    "\n",
    "- The team analyzes the current conversation context, including the conversation history and participants’ name and description attributes, to determine the next speaker using a model. You can override the model by providing a custom selection function.\n",
    "\n",
    "- The team prompts the selected speaker agent to provide a response, which is then broadcasted to all other participants.\n",
    "\n",
    "- The termination condition is checked to determine if the conversation should end, if not, the process repeats from step 1.\n",
    "\n",
    "- When the conversation ends, the team returns the TaskResult containing the conversation history from this task.\n",
    "\n",
    "Once the team finishes the task, the conversation context is kept within the team and all participants, so the next task can continue from the previous conversation context. You can reset the conversation context by calling `reset()`.\n",
    "\n",
    "In this section, we will demonstrate how to use `SelectorGroupChat` with a simple example for a web search and data analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search and Analysis Example\n",
    "\n",
    "### Agents\n",
    "<Image src='docs/selector-group-chat.svg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This example uses mock tools instead of real APIs for demonstration purposes\n",
    "def search_web_tool(query: str) -> str:\n",
    "    if \"2006-2007\" in query:\n",
    "        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n",
    "        Udonis Haslem: 844 points\n",
    "        Dwayne Wade: 1397 points\n",
    "        James Posey: 550 points\n",
    "        ...\n",
    "        \"\"\"\n",
    "    elif \"2007-2008\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n",
    "    elif \"2008-2009\" in query:\n",
    "        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n",
    "    return \"No data found.\"\n",
    "\n",
    "\n",
    "def percentage_change_tool(start: float, end: float) -> float:\n",
    "    return ((end - start) / start) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import AgentMessage\n",
    "from autogen_agentchat.task import Console, MaxMessageTermination, TextMentionTermination\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "\n",
    "planning_agent = AssistantAgent(\n",
    "    \"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        Web search agent: Searches for information\n",
    "        Data analyst: Performs calculations\n",
    "\n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "web_search_agent = AssistantAgent(\n",
    "    \"WebSearchAgent\",\n",
    "    description=\"A web search agent.\",\n",
    "    tools=[search_web_tool],\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a web search agent.\n",
    "    Your only tool is search_tool - use it to find information.\n",
    "    You make only one search call at a time.\n",
    "    Once you have the results, you never do calculations based on them.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "data_analyst_agent = AssistantAgent(\n",
    "    \"DataAnalystAgent\",\n",
    "    description=\"A data analyst agent. Useful for performing calculations.\",\n",
    "    model_client=model_client,\n",
    "    tools=[percentage_change_tool],\n",
    "    system_message=\"\"\"\n",
    "    You are a data analyst.\n",
    "    Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=25)\n",
    "termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await team.reset()  # Reset the team for the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n",
    "\n",
    "# Use asyncio.run(...) if you are running this in a script.\n",
    "await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Selector Function\n",
    "Often times we want better control over the selection process. To this end, we can set the `selector_func` argument with a custom selector function to override the default model-based selection. For instance, we want the Planning Agent to speak immediately after any specialized agent to check the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector_func(messages: Sequence[AgentMessage]) -> str | None:\n",
    "    if messages[-1].source != planning_agent.name:\n",
    "        return planning_agent.name\n",
    "    return None\n",
    "\n",
    "\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_func=selector_func,\n",
    ")\n",
    "\n",
    "await Console(team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `on_messages_stream()` method returns an asynchronous generator that yields each inner message generated by the agent, and the last item is the final response message in the `chat_message` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.task import Console\n",
    "\n",
    "# Use `asyncio.run(single_agent_team.reset())` when running in a script.\n",
    "await single_agent_team.reset()  # Reset the team for the next run.\n",
    "# Use `asyncio.run(single_agent_team.run_stream(task=\"What is the weather in Seattle?\"))` when running in a script.\n",
    "await Console(\n",
    "    single_agent_team.run_stream(task=\"What is the weather in Seattle?\")\n",
    ")  # Stream the messages to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Pattern\n",
    "Now we will create a team with two agents that implements the Reflection pattern, which is a multi-agent design pattern that uses a critic agent to evaluate the responses of a primary agent.\n",
    "\n",
    "See how the reflection pattern works using the Core API.\n",
    "\n",
    "In this example, we will use the `AssistantAgent` agent class for both the primary and critic agents. We will use both the `TextMentionTermination` and `MaxMessageTermination` termination conditions together to stop the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.task import MaxMessageTermination, TextMentionTermination\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Create the primary agent.\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a helpful AI assistant.\",\n",
    ")\n",
    "\n",
    "# Create the critic agent.\n",
    "critic_agent = AssistantAgent(\n",
    "    \"critic\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that stops the task if the critic approves.\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "# Define a termination condition that stops the task after 5 messages.\n",
    "max_message_termination = MaxMessageTermination(5)\n",
    "# Combine the termination conditions using the `|`` operator so that the\n",
    "# task stops when either condition is met.\n",
    "termination = text_termination | max_message_termination\n",
    "\n",
    "# Create a team with the primary and critic agents.\n",
    "reflection_team = RoundRobinGroupChat(\n",
    "    [primary_agent, critic_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"Write a short poem about fall season.\")))` when running in a script.\n",
    "await Console(\n",
    "    reflection_team.run_stream(task=\"Write a short poem about fall season.\")\n",
    ")  # Stream the messages to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming Team\n",
    "Let’s run the team again with a new task while keeping the context about the previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the poem in Chinese Tang poetry style.\n",
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"将这首诗用中文唐诗风格写一遍。\")))` when running in a script.\n",
    "await Console(reflection_team.run_stream(task=\"将这首诗用中文唐诗风格写一遍。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the poem in Spanish.\n",
    "# Use `asyncio.run(Console(reflection_team.run_stream(task=\"Write the poem in Spanish.\")))` when running in a script.\n",
    "await Console(reflection_team.run_stream(task=\"Write the poem in Spanish.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resuming A Previous Task\n",
    "We can call `run()` or `run_stream()` methods without setting the task again to resume the previous task. The team will continue from where it left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `asyncio.run(Console(reflection_team.run_stream()))` when running in a script.\n",
    "await Console(reflection_team.run_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pause for User Input\n",
    "Often times, team needs additional input from the application (i.e., user) to continue processing the task. We will show two possible ways to do it:\n",
    "\n",
    "- Set the maximum number of turns such that the team stops after the specified number of turns.\n",
    "\n",
    "- Use the `HandoffTermination` termination condition.\n",
    "\n",
    "You can also use custom termination conditions, see Termination Conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Number of Turns\n",
    "This is the simplest way to pause the team for user input. For example, you can set the maximum number of turns to 1 such that the team stops right after the first agent responds. This is useful when you want the user to constantly engage with the team, such as in a chatbot scenario.\n",
    "\n",
    "Simply set the `max_turns` parameter in the `RoundRobinGroupChat()` constructor.\n",
    "\n",
    "```python\n",
    "team = RoundRobinGroupChat([...], max_turns=1)\n",
    "```\n",
    "\n",
    "Once the team stops, the turn count will be reset. When you resume the team, it will start from 0 again.\n",
    "\n",
    "Note that `max_turn` is specific to the team class and is currently only supported by `RoundRobinGroupChat`, `SelectorGroupChat`, and `Swarm`. When used with termination conditions, the team will stop when either condition is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Handoff to Pause Team\n",
    "You can use the `HandoffTermination` termination condition to stop the team when an agent sends a `HandoffMessage` message.\n",
    "\n",
    "Let’s create a team with a single `AssistantAgent` agent with a handoff setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.agents import AssistantAgent, Handoff\n",
    "from autogen_ext.models import AzureOpenAIChatCompletionClient\n",
    "from autogen_agentchat.task import HandoffTermination, TextMentionTermination\n",
    "\n",
    "\n",
    "# Create an OpenAI model client.\n",
    "model_client = AzureOpenAIChatCompletionClient(\n",
    "    azure_endpoint=api_base,\n",
    "    model=\"gpt-4o\",\n",
    "    azure_deployment=\"gpt-4o-lei\",\n",
    "    api_key=api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Create a lazy assistant agent that always hands off to the user.\n",
    "lazy_agent = AssistantAgent(\n",
    "    \"lazy_assistant\",\n",
    "    model_client=model_client,\n",
    "    handoffs=[Handoff(target=\"user\", message=\"Transfer to user.\")],\n",
    "    system_message=\"Always transfer to user when you don't know the answer. Respond 'TERMINATE' when task is complete.\",\n",
    ")\n",
    "\n",
    "# Define a termination condition that checks for handoff message targetting helper and text \"TERMINATE\".\n",
    "handoff_termination = HandoffTermination(target=\"user\")\n",
    "text_termination = TextMentionTermination(\"TERMINATE\")\n",
    "termination = handoff_termination | text_termination\n",
    "\n",
    "# Create a single-agent team.\n",
    "lazy_agent_team = RoundRobinGroupChat(\n",
    "    [lazy_agent], termination_condition=termination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.task import Console\n",
    "\n",
    "# Use `asyncio.run(Console(lazy_agent_team.run_stream(task=\"What is the weather in New York?\")))` when running in a script.\n",
    "await Console(lazy_agent_team.run_stream(task=\"What is the weather in New York?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `asyncio.run(Console(lazy_agent_team.run_stream(task=\"It is raining in New York.\")))` when running in a script.\n",
    "await Console(lazy_agent_team.run_stream(task=\"It is raining in New York.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
